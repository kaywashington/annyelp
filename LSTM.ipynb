{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LPgV6DyKiPp"
      },
      "source": [
        "Install libraries etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyG5ZUFAKydI"
      },
      "outputs": [],
      "source": [
        "# https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV7R6hiFKkxn",
        "outputId": "1cf49490-0db2-483b-96c0-30f3fd2c08c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we-ZyvsyK3AQ",
        "outputId": "ab03e236-017d-4905-f875-4e04ad9f7680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "# import contractions\n",
        "# from spellchecker import SpellChecker\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "yelp_df = pd.read_csv(\"yelp.csv\")\n",
        "\n",
        "# CLEAN TEXT\n",
        "\n",
        "def clean(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  text = re.sub(r'\\W', ' ', text)\n",
        "  return text\n",
        "\n",
        "yelp_df['text'] = yelp_df['text'].apply(clean)\n",
        "\n",
        "# tokenization\n",
        "yelp_df['text'] = yelp_df['text'].apply(word_tokenize)\n",
        "\n",
        "# stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop(text):\n",
        "  return [word for word in text if word not in stop_words]\n",
        "\n",
        "yelp_df['text'] = yelp_df['text'].apply(remove_stop)\n",
        "\n",
        "# lemmatization\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lem(text):\n",
        "  return [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "yelp_df['text'] = yelp_df['text'].apply(lem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YE3FvXH3mcrm"
      },
      "outputs": [],
      "source": [
        "# change stars so that there are 3 categories\n",
        "\n",
        "def change_stars(stars):\n",
        "  if stars > 3:\n",
        "    return 2\n",
        "  elif stars < 3:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "yelp_df[\"stars\"] = yelp_df[\"stars\"].apply(change_stars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzjJ2uwNkldu",
        "outputId": "6a252032-75b6-41ce-a862-181a5e523d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              business_id        date               review_id  stars  \\\n",
            "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      2   \n",
            "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      2   \n",
            "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      2   \n",
            "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      2   \n",
            "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      2   \n",
            "\n",
            "                                                text    type  \\\n",
            "0  [wife, took, birthday, breakfast, excellent, w...  review   \n",
            "1  [idea, people, give, bad, review, place, go, s...  review   \n",
            "2  [love, gyro, plate, rice, good, also, dig, can...  review   \n",
            "3  [rosie, dakota, love, chaparral, dog, park, co...  review   \n",
            "4  [general, manager, scott, petello, good, egg, ...  review   \n",
            "\n",
            "                  user_id  cool  useful  funny  \\\n",
            "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0   \n",
            "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0   \n",
            "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0   \n",
            "3  uZetl9T0NcROGOyFfughhg     1       2      0   \n",
            "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0   \n",
            "\n",
            "                                              vector  \n",
            "0  [[0.57651, 1.1396, -0.21861, -0.28966, 1.1153,...  \n",
            "1  [[0.47564, -0.016436, -0.40476, -0.53652, 0.31...  \n",
            "2  [[-0.13886, 1.1401, -0.85212, -0.29212, 0.7553...  \n",
            "3  [[0.14394, 0.28604, -0.43377, 0.67745, 0.43447...  \n",
            "4  [[-0.23543, -0.16905, -0.32014, 0.51154, 0.954...  \n"
          ]
        }
      ],
      "source": [
        "# word embedding using glove\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# load\n",
        "def load_glove(path):\n",
        "  embeddings = {}\n",
        "  with open(path, 'r', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "      word_vector = line.strip().split()\n",
        "      word = word_vector[0]\n",
        "      vector = np.array(word_vector[1:], dtype=np.float32)\n",
        "      embeddings[word] = vector\n",
        "  return embeddings\n",
        "\n",
        "glove_path = \"glove.6B.50d.txt\"\n",
        "glove = load_glove(glove_path)\n",
        "\n",
        "# converting the vectors\n",
        "\n",
        "def text_to_vector(text, glove, dim=50):\n",
        "  vector = []\n",
        "  for word in text:\n",
        "    if word in glove:\n",
        "      vector.append(glove[word])\n",
        "    else:\n",
        "      vector.append(np.zeros(dim))\n",
        "  return vector\n",
        "\n",
        "yelp_df['vector'] = yelp_df['text'].apply(lambda text: text_to_vector(text, glove))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0ybU1IUDmf6-"
      },
      "outputs": [],
      "source": [
        "# padding\n",
        "\n",
        "max_length = 300\n",
        "\n",
        "def padding(seq):\n",
        "  if len(seq) >= max_length:\n",
        "        return np.array(seq[:max_length])\n",
        "  elif len(seq) < max_length:\n",
        "    padded = [np.zeros(50) for _ in range(max_length - len(seq))]\n",
        "    return np.array(seq + padded)\n",
        "  else:\n",
        "    print(\"else\")\n",
        "    return [np.zeros(50) for _ in range(max_length)]\n",
        "\n",
        "yelp_df[\"pad_vectors\"] = yelp_df[\"vector\"].apply(padding)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.stack(yelp_df['pad_vectors'].values)\n",
        "y = yelp_df['stars'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 47, stratify = y)\n"
      ],
      "metadata": {
        "id": "PF-3pTKYT56p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_6Ro2J0pmhp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6ed9e4-e2e3-4fce-d28c-3726ad372d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 708ms/step - accuracy: 0.4067 - loss: 1.0745 - val_accuracy: 0.5755 - val_loss: 0.9278\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 695ms/step - accuracy: 0.5701 - loss: 0.9342 - val_accuracy: 0.5555 - val_loss: 0.9095\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 693ms/step - accuracy: 0.6015 - loss: 0.8983 - val_accuracy: 0.6210 - val_loss: 0.8679\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 726ms/step - accuracy: 0.6461 - loss: 0.8172 - val_accuracy: 0.6615 - val_loss: 0.7895\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 720ms/step - accuracy: 0.6605 - loss: 0.7796 - val_accuracy: 0.5525 - val_loss: 0.9355\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 718ms/step - accuracy: 0.6794 - loss: 0.7434 - val_accuracy: 0.6170 - val_loss: 0.8492\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 712ms/step - accuracy: 0.7154 - loss: 0.6837 - val_accuracy: 0.6780 - val_loss: 0.7519\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 696ms/step - accuracy: 0.7166 - loss: 0.6320 - val_accuracy: 0.6675 - val_loss: 0.7917\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 738ms/step - accuracy: 0.7611 - loss: 0.5333 - val_accuracy: 0.5605 - val_loss: 0.9742\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 668ms/step - accuracy: 0.8002 - loss: 0.4649 - val_accuracy: 0.6625 - val_loss: 0.8611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ad8848bc710>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Masking, Dropout, Bidirectional\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0.0, input_shape=(max_length, 50)))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, class_weight=class_weights, epochs=10, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_probs = model.predict(X_test, batch_size=32, verbose=1)\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP54k42Nhc3q",
        "outputId": "967ac1cb-cc92-42a5-a0ee-f8c3ccc44b43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 196ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.59      0.55       335\n",
            "           1       0.26      0.41      0.31       292\n",
            "           2       0.87      0.73      0.80      1373\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.55      0.58      0.56      2000\n",
            "weighted avg       0.72      0.66      0.69      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import already cleaned dataset\n",
        "nyt_test = pd.read_pickle(\"nyt_df.pkl\")\n",
        "\n",
        "X_nyt = np.stack(nyt_test['pad_vectors'].values)\n",
        "y_nyt = nyt_test['stars'].values"
      ],
      "metadata": {
        "id": "F3jkkdyiQK9u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run model\n",
        "y_pred_probs_nyt = model.predict(X_nyt, batch_size=32, verbose=1)\n",
        "y_pred_nyt = np.argmax(y_pred_probs_nyt, axis=1)\n",
        "\n",
        "print(classification_report(y_nyt, y_pred_nyt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGT14d7DQhA0",
        "outputId": "55207abf-39dd-48e2-8de8-33cb90c69950"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.70      0.58        10\n",
            "           1       0.67      0.20      0.31        10\n",
            "           2       0.31      0.40      0.35        10\n",
            "\n",
            "    accuracy                           0.43        30\n",
            "   macro avg       0.49      0.43      0.41        30\n",
            "weighted avg       0.49      0.43      0.41        30\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}